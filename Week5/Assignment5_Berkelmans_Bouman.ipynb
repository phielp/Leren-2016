{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leren: Programming assignment 5\n",
    "\n",
    "**Student 1:**  <span style=\"color:red\">Wim Berkelmans</span> (<span style=\"color:red\">10793674</span>)<br>\n",
    "**Student 2:** <span style=\"color:red\">Philip Bouman</span> (<span style=\"color:red\">10668667</span>)<br>\n",
    "\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gaussian Naive Bayes\n",
    "####  a) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Gaussian Naive Bayes:  0.608333333333\n",
      "Misclassified:\n",
      "[(4.5076557255914265e-65, 156), (4.8806794970637234e-65, 149), (6.3970153534652496e-65, 207), (2.4932007457562496e-63, 152), (1.6574780653238209e-62, 157), (9.8485919284844255e-62, 222), (1.6381044703392142e-57, 105), (9.2914534931736198e-57, 132), (2.5853255454796082e-55, 126), (3.1439978732073109e-54, 101)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg as linalg\n",
    "import scipy.ndimage\n",
    "import scipy.stats as sp\n",
    "import math\n",
    "from sklearn import linear_model, datasets\n",
    "\n",
    "def readData(train,test):\n",
    "    training = np.loadtxt(train, delimiter=';')\n",
    "    test = np.loadtxt(test, delimiter=';')\n",
    "    return training, test\n",
    "\n",
    "# Calculate mean for all features in dataset\n",
    "# return list of all means\n",
    "def calcMean(X):\n",
    "    m = len(X)\n",
    "    n = len(X[0])\n",
    "    totalSum = []\n",
    "    for i in range(n):\n",
    "        sum = 0\n",
    "        for j in range(m):\n",
    "            sum += float(X[j][i])\n",
    "\n",
    "        totalSum.append(float(sum / len(X)))\n",
    "\n",
    "    return totalSum\n",
    "\n",
    "# Calculate variance for all feautures in dataset\n",
    "# based on list of means and dataset\n",
    "def calcVar(X, mean):\n",
    "    m = len(X)\n",
    "    n = len(X[0])\n",
    "    totalSum = np.ones((m,n))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            totalSum[j][i] = float((X[j][i] - mean[i])**2)\n",
    "    \n",
    "    # calc mean of variance per feature\n",
    "    var = calcMean(totalSum)\n",
    "    \n",
    "    return var\n",
    "         \n",
    "# returns the size of each class based on target values\n",
    "def classSize(Y):\n",
    "    size = []\n",
    "    uniques = list(set(Y))\n",
    "    for i in range(len(uniques)):\n",
    "        counter = 0\n",
    "        for j in range(len(Y)):\n",
    "            if Y[j] == uniques[i]:\n",
    "                counter +=1    \n",
    "        size.append(counter)\n",
    "    return size\n",
    "\n",
    "# calculate normal distribution\n",
    "def gaussian(mean, var, v):\n",
    "    if var != 0:\n",
    "        gaus = 1.0 / math.sqrt(2 * math.pi * var) * math.exp(-(v - mean)**2/(2*var))\n",
    "    else:\n",
    "        if v == mean:\n",
    "            gaus = 1\n",
    "        else:\n",
    "            gaus = 0\n",
    "            \n",
    "    return gaus\n",
    "\n",
    "# split dataset for each class\n",
    "def splitClasses(X, Y):\n",
    "    size = classSize(Y)\n",
    "    classes = []\n",
    "    class1 = X[0: size[0]]\n",
    "    classes.append(class1)\n",
    "    base = size[0]\n",
    "    for i in range(len(size)-1):\n",
    "        classN = X[base:base + size[i+1]]\n",
    "        base = base + size[i+1]\n",
    "        classes.append(classN)  \n",
    "        \n",
    "    return classes\n",
    "    \n",
    "# retrieve mean for each class\n",
    "def classMean(classes):\n",
    "    means = []\n",
    "    for i in range(len(classes)):\n",
    "        mean = calcMean(classes[i])\n",
    "        means.append(mean)\n",
    "    return means\n",
    "\n",
    "# retrieve variance for each class\n",
    "def classVars(classes, means):\n",
    "    variances = []\n",
    "    for i in range(len(classes)):\n",
    "        var = calcVar(classes[0], means[0])\n",
    "        variances.append(var)\n",
    "    return variances\n",
    "\n",
    "# claculate probabilities for each class\n",
    "def calcPDF(means, variances, testSet):\n",
    "    m = len(testSet)\n",
    "    n = len(testSet[0])-1\n",
    "    probabilities = np.ones((m,len(means)))\n",
    "    for h in range(len(means)):\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                prob = gaussian(means[h][j], variances[h][j] ,testSet[i][j])\n",
    "                if prob != 0:\n",
    "                    probabilities[i][h] *= prob\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "# give best predictions for test set\n",
    "def predict(probabilities, Y):\n",
    "    uniques = list(set(Y))\n",
    "    predictions = []\n",
    "    finalProbs = []\n",
    "    for i in range(len(probabilities)):\n",
    "        best = probabilities[i].argmax(axis=0)\n",
    "        predictions.append(uniques[best])\n",
    "        \n",
    "        probs = probabilities[i].max()\n",
    "        finalProbs.append(probs)\n",
    "        \n",
    "    return predictions, finalProbs\n",
    "    \n",
    "# calculate the percentage of correct predictions\n",
    "def getAccuracy(test_set, target):\n",
    "    correct = 0\n",
    "    miss = []\n",
    "    for i in range(len(test_set)):\n",
    "        if test_set[i][-1] == target[i]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            miss.append(i)\n",
    "            \n",
    "    accuracy = (correct/float(len(test_set))) * 100.0\n",
    "    return accuracy, miss\n",
    "\n",
    "# give back the n misclassifications with highset probability\n",
    "def highestMiss(prob, miss, n):\n",
    "    misclassified = {}\n",
    "    for i in range(len(miss)):\n",
    "        misclassified[prob[miss[i]]] = miss[i]\n",
    "    keys = sorted(misclassified.items(), key=lambda x: x[0])\n",
    "    return keys[-n:]\n",
    "    \n",
    "# main function\n",
    "def naiveBayes(X, Y, testSet):\n",
    "    # split classes and calculate mean and variance for each class\n",
    "    classes = splitClasses(X, Y)\n",
    "    means = classMean(classes)\n",
    "    variances = classVars(classes, means)\n",
    "\n",
    "    # calculate pdf, predictions and accuracy\n",
    "    probs = calcPDF(means, variances, testSet)\n",
    "    pred, finalProbs = predict(probs, Y)\n",
    "    acc, miss = getAccuracy(testSet, pred)\n",
    "    \n",
    "    # examples with highest misclassification\n",
    "    misclassified = highestMiss(finalProbs, miss, 10)\n",
    "    \n",
    "    print \"Accuracy Gaussian Naive Bayes: \", acc/100\n",
    "    print \"Misclassified:\"\n",
    "    print misclassified\n",
    "    print\n",
    "        \n",
    "# preprocess data\n",
    "dataSets = readData('digits123-1.csv','digits123-2.csv')\n",
    "X = dataSets[0]\n",
    "Y = X[:,-1]\n",
    "X = np.delete(X, -1, 1)\n",
    "testSet = dataSets[1]\n",
    "    \n",
    "naiveBayes(X, Y, testSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misclassifications\n",
    "Accuracy:  60.83 % <br>\n",
    "Ten misclassifications with highest probability (for digits123-2.csv): <br>\n",
    "101, 126, 132, 105, 222, 157, 152, 207, 149, 156"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compare classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logRegTrain(TrainX, TrainY, reg, maxI):\n",
    "    logreg = linear_model.LogisticRegression(C=reg, solver='newton-cg', \n",
    "                                    max_iter=maxI, multi_class='multinomial')\n",
    "    logreg.fit(TrainX, TrainY)\n",
    "    return logreg\n",
    "\n",
    "def logRegTest(logreg,TestX,TestY):\n",
    "    return logreg.score(TestX,TestY)\n",
    "\n",
    "\n",
    "# def logReg(Train, TrainY, Test, TestY):\n",
    "#     logreg = linear_model.LogisticRegression(C=0.001, solver='newton-cg', \n",
    "#                                     max_iter=100, multi_class='multinomial')\n",
    "#     logreg.fit(Train, TrainY)\n",
    "\n",
    "#     return logreg.score(Test,TestY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Logistic regression with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples 180\n",
      "Number of examples for cross validation 60\n",
      "Number of examples in the test set 60\n",
      "\n",
      "Accuracy Gaussian Naive Bayes:  0.716666666667\n",
      "Misclassified:\n",
      "[(1.0988338013682296e-61, 14), (9.6119803783495996e-61, 2), (2.111899565393851e-60, 30), (1.6852462784038524e-58, 48), (5.8373026747012277e-58, 4), (2.640664411379417e-57, 59), (7.9179126073727142e-57, 37), (3.3493783776435987e-56, 58), (3.2619089793241219e-55, 40), (4.0905469471446541e-55, 56)]\n",
      "\n",
      "For Logistic Regression we use sklearn\n",
      "CV Logistic Regression accuracy with regularization =  1e-05 max iterations =  20\n",
      "0.916666666667\n",
      "CV Logistic Regression accuracy with regularization =  0.0001 max iterations =  20\n",
      "0.983333333333\n",
      "CV Logistic Regression accuracy with regularization =  0.001 max iterations =  20\n",
      "1.0\n",
      "CV Logistic Regression accuracy with regularization =  0.01 max iterations =  20\n",
      "1.0\n",
      "CV Logistic Regression accuracy with regularization =  0.1 max iterations =  20\n",
      "1.0\n",
      "CV Logistic Regression accuracy with regularization =  0.001 max iterations =  200\n",
      "1.0\n",
      "Test Logistic Regression accuracy with regularization =  0.001 max iterations =  20\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# load data from file\n",
    "def readDigits(file):\n",
    "    data = np.loadtxt(file, delimiter=';')  \n",
    "    return data\n",
    "\n",
    "def init():\n",
    "    Digits = readDigits('digits123-1.csv')\n",
    "    # divide input into training set (60%), crossvalidation set (20%) and test set (20%)\n",
    "    m = len(Digits)\n",
    "    CV = Digits[::5]\n",
    "    Test = Digits[1::5]\n",
    "    Train1 = Digits[2::5]\n",
    "    Train2 = Digits[3::5]\n",
    "    Train3 = Digits[4::5]\n",
    "    Train = np.concatenate((Train1,Train2,Train3))\n",
    "    return Train, CV, Test\n",
    "\n",
    "Train, CV, Test = init()\n",
    "print 'Number of training examples',len(Train)\n",
    "print 'Number of examples for cross validation',len(CV)\n",
    "print 'Number of examples in the test set',len(Test)\n",
    "print \n",
    "\n",
    "TrainY = Train[:,-1] # Y target values, last column\n",
    "TrainX = np.delete(Train, -1, 1) # remove target values\n",
    "CVY = CV[:,-1] # Y target values, last column\n",
    "CVX = np.delete(CV, -1, 1) # remove target values\n",
    "TestY = Test[:,-1] # Y target values, last column\n",
    "TestX = np.delete(Test, -1, 1) # remove target values\n",
    "\n",
    "# run Naive Bayes\n",
    "naiveBayes(TrainX, TrainY, Test)\n",
    "\n",
    "print \"For Logistic Regression we use sklearn\"\n",
    "# train logistic regression\n",
    "reg = 0.00001\n",
    "maxI = 20\n",
    "logreg = logRegTrain(TrainX, TrainY, reg, maxI)\n",
    "# validate the parameters\n",
    "print \"CV Logistic Regression accuracy with regularization = \", reg, \"max iterations = \",maxI\n",
    "print logRegTest(logreg, CVX, CVY)\n",
    "# train logistic regression\n",
    "reg = 0.0001\n",
    "maxI = 20\n",
    "logreg = logRegTrain(TrainX, TrainY, reg, maxI)\n",
    "# validate the parameters\n",
    "print \"CV Logistic Regression accuracy with regularization = \", reg, \"max iterations = \",maxI\n",
    "print logRegTest(logreg, CVX, CVY)\n",
    "# train logistic regression\n",
    "reg = 0.001\n",
    "maxI = 20\n",
    "logreg = logRegTrain(TrainX, TrainY, reg, maxI)\n",
    "# validate the parameters\n",
    "print \"CV Logistic Regression accuracy with regularization = \", reg, \"max iterations = \",maxI\n",
    "print logRegTest(logreg, CVX, CVY)\n",
    "# train logistic regression\n",
    "reg = 0.01\n",
    "maxI = 20\n",
    "logreg = logRegTrain(TrainX, TrainY, reg, maxI)\n",
    "# validate the parameters\n",
    "print \"CV Logistic Regression accuracy with regularization = \", reg, \"max iterations = \",maxI\n",
    "print logRegTest(logreg, CVX, CVY)\n",
    "# train logistic regression\n",
    "reg = 0.1\n",
    "maxI = 20\n",
    "logreg = logRegTrain(TrainX, TrainY, reg, maxI)\n",
    "# validate the parameters\n",
    "print \"CV Logistic Regression accuracy with regularization = \", reg, \"max iterations = \",maxI\n",
    "print logRegTest(logreg, CVX, CVY)\n",
    "# train logistic regression\n",
    "reg = 0.001\n",
    "maxI = 200\n",
    "logreg = logRegTrain(TrainX, TrainY, reg, maxI)\n",
    "# validate the parameters\n",
    "print \"CV Logistic Regression accuracy with regularization = \", reg, \"max iterations = \",maxI\n",
    "print logRegTest(logreg, CVX, CVY)\n",
    "# test the model\n",
    "reg = 0.001\n",
    "maxI = 20\n",
    "logreg = logRegTrain(TrainX, TrainY, reg, maxI)\n",
    "print \"Test Logistic Regression accuracy with regularization = \", reg, \"max iterations = \",maxI\n",
    "print logRegTest(logreg, TestX, TestY)\n",
    "# print logReg(TrainX, TrainY, TestX, TestY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Logistic regression is much more accurate than Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
